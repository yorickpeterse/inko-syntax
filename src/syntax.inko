# A simple syntax highlighting library for Inko.
import std.cmp.Equal
import std.fmt.(Format, Formatter)
import std.hash.(Hash, Hasher)
import std.iter.Iter
import syntax.helpers.(LF, whitespace?)

# The value returned by `Lexer.current` and `Lexer.peek` to signal the end of
# the input is reached.
let pub EOF = -1

class pub enum TokenKind {
  # A single or multi-line comment.
  case Comment

  # A double quoted string.
  case DoubleString

  # A single quoted string.
  case SingleString

  # An integer.
  case Int

  # A float (e.g. `10.5` or `10e5`).
  case Float

  # Regular text, such as unrecognized/highlighted words or whitespace.
  case Text

  # A keyword, such as `class`.
  case Keyword

  # A reference to a field, such as `self.foo` in Rust or `@foo` in Inko.
  case Field
}

impl Hash for TokenKind {
  fn pub hash[H: mut + Hasher](hasher: mut H) {
    let val = match self {
      case Comment -> 0
      case DoubleString -> 1
      case SingleString -> 2
      case Int -> 3
      case Float -> 4
      case Text -> 5
      case Keyword -> 6
      case Field -> 7
    }

    hasher.write(val)
  }
}

impl Equal[TokenKind] for TokenKind {
  fn pub ==(other: ref TokenKind) -> Bool {
    match (self, other) {
      case (Comment, Comment) -> true
      case (DoubleString, DoubleString) -> true
      case (SingleString, SingleString) -> true
      case (Int, Int) -> true
      case (Float, Float) -> true
      case (Text, Text) -> true
      case (Keyword, Keyword) -> true
      case (Field, Field) -> true
      case _ -> false
    }
  }
}

impl Format for TokenKind {
  fn pub fmt(formatter: mut Formatter) {
    match self {
      case Comment -> formatter.tuple('Comment').finish
      case DoubleString -> formatter.tuple('DoubleString').finish
      case SingleString -> formatter.tuple('SingleString').finish
      case Int -> formatter.tuple('Int').finish
      case Float -> formatter.tuple('Float').finish
      case Text -> formatter.tuple('Text').finish
      case Keyword -> formatter.tuple('Keyword').finish
      case Field -> formatter.tuple('Field').finish
    }
  }
}

# A token to highlight.
class pub Token {
  # The kind/type of token.
  let pub @kind: TokenKind

  # The byte offset the value of the token starts at.
  let pub @start: Int

  # The number of bytes the value of this token contains.
  let pub @size: Int

  # Returns a new `Token`.
  fn pub static new(kind: TokenKind, start: Int, size: Int) -> Token {
    Token { @kind = kind, @start = start, @size = size }
  }
}

impl Equal[Token] for Token {
  fn pub ==(other: ref Token) -> Bool {
    @kind == other.kind and @start == other.start and @size == other.size
  }
}

impl Format for Token {
  fn pub fmt(formatter: mut Formatter) {
    formatter
      .object('Token')
      .field('kind', @kind)
      .field('start', @start)
      .field('size', @size)
      .finish
  }
}

# A type that represents a bunch of bytes to process using a lexer.
class Buffer {
  let @bytes: ref ByteArray
  let @offset: Int

  fn static new(bytes: ref ByteArray) -> Buffer {
    Buffer { @bytes = bytes, @offset = 0 }
  }

  fn size -> Int {
    @bytes.size
  }

  fn get -> Int {
    peek(0)
  }

  fn peek(amount: Int) -> Int {
    let idx = @offset + amount

    if idx < @bytes.size { @bytes.byte(idx) } else { EOF }
  }

  fn slice(start: Int, stop: Int) -> String {
    @bytes.slice(start, stop - start).into_string
  }

  fn mut advance_until_eol {
    while get != EOF and get != LF { @offset += 1 }
  }

  fn mut advance_whitespace {
    while whitespace?(get) { @offset += 1 }
  }

  fn token(kind: TokenKind, start: Int) -> Token {
    Token.new(kind, start, size: @offset - start)
  }
}
