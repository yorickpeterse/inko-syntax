# A simple syntax highlighting library for Inko.
import std.cmp.Equal
import std.fmt.(Format, Formatter)
import std.iter.Iter
import std.string.Bytes
import syntax.helpers.(LF, whitespace?)

# The value returned by `Lexer.current` and `Lexer.peek` to signal the end of
# the input is reached.
let pub EOF = -1

# A token to highlight.
#
# A token can be a comment, a string, a keyword, or just regular text.
class pub enum Token {
  # A token produced as a result of invalid syntax.
  #
  # The wrapped `Int` is the byte offset at which the error is encountered.
  case Invalid(Int)

  # A single or multi-line comment.
  case Comment(String)

  # A double quoted string.
  case DoubleString(String)

  # A single quoted string.
  case SingleString(String)

  # An integer.
  case Int(String)

  # A float (e.g. `10.5` or `10e5`).
  case Float(String)

  # Regular text, such as unrecognized/highlighted words or whitespace.
  case Text(String)

  # A keyword, such as `class`.
  case Keyword(String)

  # A reference to a field, such as `self.foo` in Rust or `@foo` in Inko.
  case Field(String)
}

impl Equal[Token] for Token {
  fn pub ==(other: ref Token) -> Bool {
    match (self, other) {
      case (Invalid(a), Invalid(b)) -> a == b
      case (Comment(a), Comment(b)) -> a == b
      case (DoubleString(a), DoubleString(b)) -> a == b
      case (SingleString(a), SingleString(b)) -> a == b
      case (Int(a), Int(b)) -> a == b
      case (Float(a), Float(b)) -> a == b
      case (Text(a), Text(b)) -> a == b
      case (Keyword(a), Keyword(b)) -> a == b
      case (Field(a), Field(b)) -> a == b
      case _ -> false
    }
  }
}

impl Format for Token {
  fn pub fmt(formatter: mut Formatter) {
    match self {
      case Invalid(v) -> formatter.tuple('Invalid').field(v).finish
      case Comment(v) -> formatter.tuple('Comment').field(v).finish
      case DoubleString(v) -> formatter.tuple('DoubleString').field(v).finish
      case SingleString(v) -> formatter.tuple('SingleString').field(v).finish
      case Int(v) -> formatter.tuple('Int').field(v).finish
      case Float(v) -> formatter.tuple('Float').field(v).finish
      case Text(v) -> formatter.tuple('Text').field(v).finish
      case Keyword(v) -> formatter.tuple('Keyword').field(v).finish
      case Field(v) -> formatter.tuple('Field').field(v).finish
    }
  }
}

# A type that turns source code into a stream of `Token` values.
trait pub Lexer: Iter[Token] {
  # Returns the bytes to lex.
  fn pub bytes -> ref Bytes

  # Returns the current byte offset.
  fn pub offset -> Int

  # Advances the cursor to the next byte.
  fn pub mut advance

  # Returns the byte at the current byte offset.
  #
  # If all bytes are consumed, this method returns `-1`
  fn pub current -> Int {
    if offset < bytes.size { bytes.byte(offset) } else { EOF }
  }

  # Returns the byte at the next byte offset.
  #
  # If all bytes are consumed, this method returns `-1`
  fn pub peek -> Int {
    let idx = offset + 1

    if idx < bytes.size { bytes.byte(idx) } else { EOF }
  }

  # Slices the range of bytes into a `String`.
  fn pub slice(start: Int, stop: Int) -> String {
    bytes.slice(start, stop - start).into_string
  }

  # Advances the byte offset until the end of the current line
  fn pub mut advance_until_eol {
    while current != EOF and current != LF { advance }
  }

  # Advances the byte offset until the first non-whitespace byte.
  fn pub mut advance_whitespace {
    while whitespace?(current) { advance }
  }

  # Returns an invalid token for the current offset, and advances the offset to
  # the next byte.
  fn pub mut invalid -> Token {
    advance
    Token.Invalid(offset - 1)
  }
}
