# A simple syntax highlighting library for Inko.
import std.cmp.Equal
import std.fmt.(Format as Format, Formatter)
import std.hash.(Hash, Hasher)
import std.iter.Iter
import std.set.Set
import syntax.bytes.(BSLASH, LF, whitespace?)
import syntax.format
import syntax.lexer.c
import syntax.lexer.fish
import syntax.lexer.inko
import syntax.lexer.make
import syntax.lexer.ruby
import syntax.lexer.rust
import syntax.lexer.shell
import syntax.lexer.toml

# The value returned by `Lexer.current` and `Lexer.peek` to signal the end of
# the input is reached.
let pub EOF = -1

class pub enum TokenKind {
  # A single or multi-line comment.
  case Comment

  # A string.
  case String

  # An integer.
  case Int

  # A float (e.g. `10.5` or `10e5`).
  case Float

  # Regular text, such as unrecognized/highlighted words or whitespace.
  case Text

  # A keyword, such as `class`.
  case Keyword

  # A reference to a field, such as `self.foo` in Rust or `@foo` in Inko.
  case Field

  # A symbol/atom, such as `:foo` in Ruby.
  case Symbol

  # A macro definition or call (i.e. `vec![...]` in Rust or a `#define` in C).
  case Macro
}

impl Hash for TokenKind {
  fn pub hash[H: mut + Hasher](hasher: mut H) {
    let val = match self {
      case Comment -> 0
      case String -> 1
      case Int -> 2
      case Float -> 3
      case Text -> 4
      case Keyword -> 5
      case Field -> 6
      case Symbol -> 7
      case Macro -> 8
    }

    hasher.write(val)
  }
}

impl Equal[ref TokenKind] for TokenKind {
  fn pub ==(other: ref TokenKind) -> Bool {
    match (self, other) {
      case (Comment, Comment) -> true
      case (String, String) -> true
      case (Int, Int) -> true
      case (Float, Float) -> true
      case (Text, Text) -> true
      case (Keyword, Keyword) -> true
      case (Field, Field) -> true
      case (Symbol, Symbol) -> true
      case (Macro, Macro) -> true
      case _ -> false
    }
  }
}

impl Format for TokenKind {
  fn pub fmt(formatter: mut Formatter) {
    match self {
      case Comment -> formatter.tuple('Comment').finish
      case String -> formatter.tuple('String').finish
      case Int -> formatter.tuple('Int').finish
      case Float -> formatter.tuple('Float').finish
      case Text -> formatter.tuple('Text').finish
      case Keyword -> formatter.tuple('Keyword').finish
      case Field -> formatter.tuple('Field').finish
      case Symbol -> formatter.tuple('Symbol').finish
      case Macro -> formatter.tuple('Macro').finish
    }
  }
}

# A token to highlight.
class pub Token {
  # The kind/type of token.
  let pub @kind: TokenKind

  # The value of the token.
  let pub @value: String

  # Returns a new `Token`.
  fn pub static new(kind: TokenKind, value: String) -> Token {
    Token { @kind = kind, @value = value }
  }
}

impl Equal[ref Token] for Token {
  fn pub ==(other: ref Token) -> Bool {
    @kind == other.kind and @value == other.value
  }
}

impl Format for Token {
  fn pub fmt(formatter: mut Formatter) {
    formatter.object('Token').field('kind', @kind).field('value', @value).finish
  }
}

# A type that turns a buffer into a token stream.
trait pub Lexer: Iter[Token] {
  # The buffer to turn into a token stream.
  fn pub mut buffer -> mut Buffer
}

# A type to efficiently check if certain words are keywords.
#
# This type only supports keywords with a minimum size of two bytes. Input
# smaller than two bytes is never treated as a keyword.
class pub Keywords {
  let @words: Set[ByteArray]
  let @first: ByteArray
  let @second: ByteArray

  # Returns a new `Keywords` populated using the list of keywords.
  #
  # # Examples
  #
  #     import syntax.Keywords
  #
  #     Keywords.new(['fn', 'class', 'async'])
  fn pub static new(keywords: ref Array[String]) -> Keywords {
    let words = Set.new
    let first = ByteArray.filled(with: 0, times: 255)
    let second = ByteArray.filled(with: 0, times: 255)

    keywords.iter.select fn (w) { w.size >= 2 }.each fn (word) {
      let bytes = word.to_byte_array

      first.set(bytes.get(0), 1)
      second.set(bytes.get(1), 1)
      words.insert(bytes)
    }

    Keywords { @words = words, @first = first, @second = second }
  }

  # Returns true if the range of bytes in `bytes` is a valid keyword.
  #
  # In the best case scenario, this method doesn't perform any allocations (e.g.
  # the first two bytes don't match any known keywords). In the worst case it
  # slices `bytes` into a subslice and compares that to the known list of
  # keywords.
  #
  # The `bytes` argument is a `ByteArray` (e.g. a reusable buffer) to check.
  #
  # The `at` and `size` arguments specify the index to `bytes` and the number of
  # bytes to check respectively.
  #
  # # Panics
  #
  # This method panics if `at` or `at + size` is not within the bounds of
  # `bytes`.
  #
  # # Examples
  #
  #     import syntax.Keywords
  #
  #     let kw = Keywords.new(['class', 'async', 'fn'])
  #     let input = 'class'.to_byte_array
  #
  #     kw.contains_range?(input, at: 0, size: input.size) # => true
  fn pub contains_range?(bytes: ref ByteArray, at: Int, size: Int) -> Bool {
    size >= 2
      and @first.get(bytes.get(at)) == 1
      and @second.get(bytes.get(at + 1)) == 1
      and @words.contains?(bytes.slice(at, size))
  }
}

# A type describing a language, such as its file extensions and names.
class pub Language {
  # The identifier/primary name of the language.
  #
  # This name should be a lowercase alphanumerical name, such that it's valid
  # for e.g. HTML classes.
  let pub @name: String

  # Any aliases the language is available under as well.
  let pub @aliases: Array[String]

  # A closure used for constructing a lexer.
  #
  # The closure's argument is the `ByteArray` to turn into a token stream.
  #
  # Note that the closure is expected to return a value typed as the `Lexer`
  # trait. This means you'll need to explicitly cast any lexer _classes_ to the
  # trait using `... as Lexer`.
  let pub @builder: fn (ref ByteArray) -> Lexer

  # Creates a new `Lexer` that processes the given `ByteArray`.
  fn pub mut lexer(bytes: ref ByteArray) -> Lexer {
    @builder.call(bytes)
  }
}

# A type that represents a bunch of bytes to process using a lexer.
class pub Buffer {
  # The bytes that are to be turned into a token stream.
  let pub @bytes: ref ByteArray

  # The current byte offset.
  let pub @offset: Int

  # Returns a new `Buffer` that wraps the given `ByteArray`.
  fn pub static new(bytes: ref ByteArray) -> Buffer {
    Buffer { @bytes = bytes, @offset = 0 }
  }

  # Returns the total number of bytes in the buffer.
  fn pub size -> Int {
    @bytes.size
  }

  # Returns the byte at the current byte offset.
  fn pub get -> Int {
    peek(0)
  }

  # Returns the byte that is at `amount` bytes relative to the current offset.
  #
  # If the offset is out of range, the `EOF` byte is returned.
  fn pub peek(amount: Int) -> Int {
    let idx = @offset + amount

    if idx < @bytes.size { @bytes.byte(idx) } else { EOF }
  }

  # Slices the buffer into a substring.
  fn pub slice(start: Int, size: Int) -> String {
    @bytes.slice(start, size).into_string
  }

  # Advances the cursor while the given closure returns `true`.
  fn pub mut advance_while(condition: fn (Int) -> Bool) {
    while @offset < @bytes.size and condition.call(@bytes.byte(@offset)) {
      @offset += 1
    }
  }

  # Returns a new token starting at the given offset that ranges until the
  # current offset.
  fn pub token(kind: TokenKind, start: Int) -> Token {
    Token.new(kind, value: slice(start, @offset - start))
  }
}

# A registry of languages and their lexers.
#
# A `Languages` registry is useful for consumers that want to support multiple
# languages, without having to map language names to the corresponding lexers
# themselves.
#
# # Examples
#
# Getting a language using a name, and creating its lexer:
#
#     import syntax.Languages
#
#     let langs = Languages.new
#
#     langs.for_name('inko').unwrap.create('# test'.to_byte_array)
#
# Getting a language using a file name:
#
#     import syntax.Languages
#
#     let langs = Languages.new
#
#     langs.for_filename('*.inko').unwrap.create('# test'.to_byte_array)
class pub Languages {
  let @aliases: Map[String, mut Language]
  let @map: Map[String, Language]

  # Returns a new registry, containing all default lexers.
  fn pub static new -> Languages {
    let langs = empty

    c.register(langs)
    fish.register(langs)
    inko.register(langs)
    make.register(langs)
    ruby.register(langs)
    rust.register(langs)
    shell.register(langs)
    toml.register(langs)
    langs
  }

  # Returns a new empty registry.
  #
  # You'll probably want to use `Languages.new` in most cases. This method is
  # meant for cases where you only want a subset of the available languages, and
  # are OK with registering them yourself.
  fn pub static empty -> Languages {
    Languages { @aliases = Map.new, @map = Map.new, }
  }

  # Registers a new language.
  fn pub mut add(language: Language) {
    language.aliases.iter.each fn (name) { @aliases.set(name, language) }
    @map.set(language.name, language)
  }

  # Returns the language for the given name.
  #
  # If the name isn't recognized, a `None` is returned.
  fn pub mut get(name: String) -> Option[mut Language] {
    @map.opt_mut(name).else fn { @aliases.opt_mut(name) }
  }
}
