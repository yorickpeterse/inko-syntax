# A simple syntax highlighting library for Inko.
import std.cmp.Equal
import std.fmt.(Format as Format, Formatter)
import std.hash.(Hash, Hasher)
import std.iter.Iter
import syntax.format
import syntax.helpers.(LF, whitespace?)
import syntax.lexers.inko

# The value returned by `Lexer.current` and `Lexer.peek` to signal the end of
# the input is reached.
let pub EOF = -1

class pub enum TokenKind {
  # A single or multi-line comment.
  case Comment

  # A double quoted string.
  case DoubleString

  # A single quoted string.
  case SingleString

  # An integer.
  case Int

  # A float (e.g. `10.5` or `10e5`).
  case Float

  # Regular text, such as unrecognized/highlighted words or whitespace.
  case Text

  # A keyword, such as `class`.
  case Keyword

  # A reference to a field, such as `self.foo` in Rust or `@foo` in Inko.
  case Field
}

impl Hash for TokenKind {
  fn pub hash[H: mut + Hasher](hasher: mut H) {
    let val = match self {
      case Comment -> 0
      case DoubleString -> 1
      case SingleString -> 2
      case Int -> 3
      case Float -> 4
      case Text -> 5
      case Keyword -> 6
      case Field -> 7
    }

    hasher.write(val)
  }
}

impl Equal[TokenKind] for TokenKind {
  fn pub ==(other: ref TokenKind) -> Bool {
    match (self, other) {
      case (Comment, Comment) -> true
      case (DoubleString, DoubleString) -> true
      case (SingleString, SingleString) -> true
      case (Int, Int) -> true
      case (Float, Float) -> true
      case (Text, Text) -> true
      case (Keyword, Keyword) -> true
      case (Field, Field) -> true
      case _ -> false
    }
  }
}

impl Format for TokenKind {
  fn pub fmt(formatter: mut Formatter) {
    match self {
      case Comment -> formatter.tuple('Comment').finish
      case DoubleString -> formatter.tuple('DoubleString').finish
      case SingleString -> formatter.tuple('SingleString').finish
      case Int -> formatter.tuple('Int').finish
      case Float -> formatter.tuple('Float').finish
      case Text -> formatter.tuple('Text').finish
      case Keyword -> formatter.tuple('Keyword').finish
      case Field -> formatter.tuple('Field').finish
    }
  }
}

# A token to highlight.
class pub Token {
  # The kind/type of token.
  let pub @kind: TokenKind

  # The byte offset the value of the token starts at.
  let pub @start: Int

  # The number of bytes the value of this token contains.
  let pub @size: Int

  # Returns a new `Token`.
  fn pub static new(kind: TokenKind, start: Int, size: Int) -> Token {
    Token { @kind = kind, @start = start, @size = size }
  }
}

impl Equal[Token] for Token {
  fn pub ==(other: ref Token) -> Bool {
    @kind == other.kind and @start == other.start and @size == other.size
  }
}

impl Format for Token {
  fn pub fmt(formatter: mut Formatter) {
    formatter
      .object('Token')
      .field('kind', @kind)
      .field('start', @start)
      .field('size', @size)
      .finish
  }
}

# A type that turns a buffer into a token stream.
trait pub Lexer: Iter[Token] {
  # The buffer to turn into a token stream.
  fn pub mut buffer -> mut Buffer
}

# A type describing a language, such as its file extensions and names.
class pub Language {
  # The identifier/primary name of the language.
  #
  # This name must follow the pattern `/[a-zA-Z0-9_]+/`
  let pub @name: String

  # Any aliases the language is available under as well.
  let pub @aliases: Array[String]

  # The file extension(s) (minus the leading dot) of the language.
  let pub @extensions: Array[String]

  # A closure used for constructing a lexer.
  #
  # The closure's argument is the `ByteArray` to turn into a token stream.
  #
  # Note that the closure is expected to return a value typed as the `Lexer`
  # trait. This means you'll need to explicitly cast any lexer _classes_ to the
  # trait using `... as Lexer`.
  let pub @builder: fn (ref ByteArray) -> Lexer

  # Creates a new `Lexer` that processes the given `ByteArray`.
  fn pub mut lexer(bytes: ref ByteArray) -> Lexer {
    @builder.call(bytes)
  }
}

# A type that represents a bunch of bytes to process using a lexer.
class pub Buffer {
  # The bytes that are to be turned into a token stream.
  let pub @bytes: ref ByteArray

  # The current byte offset.
  let pub @offset: Int

  # Returns a new `Buffer` that wraps the given `ByteArray`.
  fn pub static new(bytes: ref ByteArray) -> Buffer {
    Buffer { @bytes = bytes, @offset = 0 }
  }

  # Returns the total number of bytes in the buffer.
  fn pub size -> Int {
    @bytes.size
  }

  # Returns the byte at the current byte offset.
  fn pub get -> Int {
    peek(0)
  }

  # Returns the byte that is at `amount` bytes relative to the current offset.
  #
  # If the offset is out of range, the `EOF` byte is returned.
  fn pub peek(amount: Int) -> Int {
    let idx = @offset + amount

    if idx < @bytes.size { @bytes.byte(idx) } else { EOF }
  }

  # Slices the buffer into a substring.
  fn pub slice(start: Int, size: Int) -> String {
    @bytes.slice(start, size).into_string
  }

  # Advances the cursor until reaching the end of the current line.
  fn pub mut advance_until_eol {
    while get != EOF and get != LF { @offset += 1 }
  }

  # Advances the cursor until reaching the first non-whitespace byte.
  fn pub mut advance_whitespace {
    while whitespace?(get) { @offset += 1 }
  }

  # Returns a new token starting at the given offset that ranges until the
  # current offset.
  fn pub token(kind: TokenKind, start: Int) -> Token {
    Token.new(kind, start, size: @offset - start)
  }
}

# A registry of languages and their lexers.
#
# A `Languages` registry is useful for consumers that want to support multiple
# languages, without having to map language names to the corresponding lexers
# themselves.
#
# # Examples
#
# Getting a language using a name, and creating its lexer:
#
#     import syntax.Languages
#
#     let langs = Languages.new
#
#     langs.for_name('inko').unwrap.create('# test'.to_byte_array)
#
# Getting a language using a file extension:
#
#     import syntax.Languages
#
#     let langs = Languages.new
#
#     langs.for_extension('inko').unwrap.create('# test'.to_byte_array)
class pub Languages {
  let @aliases: Map[String, mut Language]
  let @extensions: Map[String, mut Language]
  let @map: Map[String, Language]

  # Returns a new registry, containing all default lexers.
  fn pub static new -> Languages {
    let langs = empty

    inko.register(langs)
    langs
  }

  # Returns a new empty registry.
  #
  # You'll probably want to use `Languages.new` in most cases. This method is
  # meant for cases where you only want a subset of the available languages, and
  # are OK with registering them yourself.
  fn pub static empty -> Languages {
    Languages {
      @aliases = Map.new,
      @extensions = Map.new,
      @map = Map.new,
    }
  }

  # Registers a new language.
  fn pub mut add(language: Language) {
    language.aliases.iter.each fn (name) { @aliases.set(name, language) }
    language.extensions.iter.each fn (name) { @extensions.set(name, language) }
    @map.set(language.name, language)
  }

  # Returns the language for the given name.
  #
  # If the name isn't recognized, a `None` is returned.
  fn pub mut for_name(name: String) -> Option[mut Language] {
    @map.opt_mut(name).else fn { @aliases.opt_mut(name) }
  }

  # Returns the language for the given file extension.
  #
  # If the extension isn't recognized, a `None` is returned.
  fn pub mut for_extension(name: String) -> Option[mut Language] {
    @extensions.opt_mut(name)
  }
}
