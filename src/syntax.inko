# A simple syntax highlighting library for Inko.
import std.cmp.Equal
import std.fmt.(Format as Format, Formatter)
import std.hash.(Hash, Hasher)
import std.iter.Iter
import syntax.format
import syntax.helpers.(LF, whitespace?)
import syntax.lexers.inko

# The value returned by `Lexer.current` and `Lexer.peek` to signal the end of
# the input is reached.
let pub EOF = -1

class pub enum TokenKind {
  # A single or multi-line comment.
  case Comment

  # A double quoted string.
  case DoubleString

  # A single quoted string.
  case SingleString

  # An integer.
  case Int

  # A float (e.g. `10.5` or `10e5`).
  case Float

  # Regular text, such as unrecognized/highlighted words or whitespace.
  case Text

  # A keyword, such as `class`.
  case Keyword

  # A reference to a field, such as `self.foo` in Rust or `@foo` in Inko.
  case Field
}

impl Hash for TokenKind {
  fn pub hash[H: mut + Hasher](hasher: mut H) {
    let val = match self {
      case Comment -> 0
      case DoubleString -> 1
      case SingleString -> 2
      case Int -> 3
      case Float -> 4
      case Text -> 5
      case Keyword -> 6
      case Field -> 7
    }

    hasher.write(val)
  }
}

impl Equal[TokenKind] for TokenKind {
  fn pub ==(other: ref TokenKind) -> Bool {
    match (self, other) {
      case (Comment, Comment) -> true
      case (DoubleString, DoubleString) -> true
      case (SingleString, SingleString) -> true
      case (Int, Int) -> true
      case (Float, Float) -> true
      case (Text, Text) -> true
      case (Keyword, Keyword) -> true
      case (Field, Field) -> true
      case _ -> false
    }
  }
}

impl Format for TokenKind {
  fn pub fmt(formatter: mut Formatter) {
    match self {
      case Comment -> formatter.tuple('Comment').finish
      case DoubleString -> formatter.tuple('DoubleString').finish
      case SingleString -> formatter.tuple('SingleString').finish
      case Int -> formatter.tuple('Int').finish
      case Float -> formatter.tuple('Float').finish
      case Text -> formatter.tuple('Text').finish
      case Keyword -> formatter.tuple('Keyword').finish
      case Field -> formatter.tuple('Field').finish
    }
  }
}

# A token to highlight.
class pub Token {
  # The kind/type of token.
  let pub @kind: TokenKind

  # The byte offset the value of the token starts at.
  let pub @start: Int

  # The number of bytes the value of this token contains.
  let pub @size: Int

  # Returns a new `Token`.
  fn pub static new(kind: TokenKind, start: Int, size: Int) -> Token {
    Token { @kind = kind, @start = start, @size = size }
  }
}

impl Equal[Token] for Token {
  fn pub ==(other: ref Token) -> Bool {
    @kind == other.kind and @start == other.start and @size == other.size
  }
}

impl Format for Token {
  fn pub fmt(formatter: mut Formatter) {
    formatter
      .object('Token')
      .field('kind', @kind)
      .field('start', @start)
      .field('size', @size)
      .finish
  }
}

# A type that turns a buffer into a token stream.
trait pub Lexer: Iter[Token] {
  # The buffer to turn into a token stream.
  fn pub mut buffer -> mut Buffer

  # Returns an identifier describing the name of the language this lexer
  # processes.
  #
  # The name shouldn't contain any spaces, and be suitable for use in e.g. the
  # "class" attribute of an HTML element.
  fn pub language_identifier -> String
}

# A type that represents a bunch of bytes to process using a lexer.
class pub Buffer {
  # The bytes that are to be turned into a token stream.
  let pub @bytes: ref ByteArray

  # The current byte offset.
  let pub @offset: Int

  # Returns a new `Buffer` that wraps the given `ByteArray`.
  fn pub static new(bytes: ref ByteArray) -> Buffer {
    Buffer { @bytes = bytes, @offset = 0 }
  }

  # Returns the total number of bytes in the buffer.
  fn pub size -> Int {
    @bytes.size
  }

  # Returns the byte at the current byte offset.
  fn pub get -> Int {
    peek(0)
  }

  # Returns the byte that is at `amount` bytes relative to the current offset.
  #
  # If the offset is out of range, the `EOF` byte is returned.
  fn pub peek(amount: Int) -> Int {
    let idx = @offset + amount

    if idx < @bytes.size { @bytes.byte(idx) } else { EOF }
  }

  # Slices the buffer into a substring.
  fn pub slice(start: Int, size: Int) -> String {
    @bytes.slice(start, size).into_string
  }

  # Advances the cursor until reaching the end of the current line.
  fn pub mut advance_until_eol {
    while get != EOF and get != LF { @offset += 1 }
  }

  # Advances the cursor until reaching the first non-whitespace byte.
  fn pub mut advance_whitespace {
    while whitespace?(get) { @offset += 1 }
  }

  # Returns a new token starting at the given offset that ranges until the
  # current offset.
  fn pub token(kind: TokenKind, start: Int) -> Token {
    Token.new(kind, start, size: @offset - start)
  }
}

# A registry of languages and their lexers.
#
# A `Lexers` registry is useful for consumers that want to support multiple
# languages, without having to map language names to the corresponding lexers
# themselves.
#
# # Examples
#
# Creating a registered lexer:
#
#     import syntax.Lexers
#
#     let lexers = Lexers.new
#
#     lexers.create('inko', '# test'.to_byte_array).unwrap
#
# Adding a lexer to the registry:
#
#     import syntax.Lexers
#     import syntax.lexers.inko
#
#     let lexers = Lexers.new
#
#     lexers.add('inko') fn (in) { inko.Lexer.new(in) as Lexer }
class pub Lexers {
  let @map: Map[String, fn (ref ByteArray) -> Lexer]

  # Returns a new lexers registry, containing all default lexers.
  fn pub static new -> Lexers {
    let map = Map.new

    map.set('inko') fn (in) { inko.Lexer.new(in) as Lexer }
    Lexers { @map = map }
  }

  # Registers a new language and lexer.
  #
  # Note that the `builder` closure is expected to return a value typed as the
  # `Lexer` trait. This means you'll need to explicitly cast any lexer _classes_
  # to the trait using `... as Lexer`.
  fn pub mut add(language: String, builder: fn (ref ByteArray) -> Lexer) {
    @map.set(language, builder)
  }

  # Creates a new lexer for the given language and input.
  #
  # The `language` argument specifies the language to create the lexer for. If
  # the language isn't registered, a `None` is returned.
  #
  # The `bytes` argument is an immutable reference to the bytes to turn into a
  # token stream.
  fn pub mut create(language: String, bytes: ref ByteArray) -> Option[Lexer] {
    @map.opt_mut(language).map fn (f) { f.call(bytes) }
  }
}
