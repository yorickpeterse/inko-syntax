import std.test (Tests)
import syntax (Token, TokenKind)
import syntax.lexer.c (KEYWORDS, Lexer)

fn lex(input: String) -> Array[Token] {
  Lexer.new(input.to_byte_array).to_array
}

fn pub tests(t: mut Tests) {
  t.test('Strings', fn (t) {
    t.equal(lex('""'), [Token.new(TokenKind.String, '""')])
    t.equal(lex('"foo"'), [Token.new(TokenKind.String, '"foo"')])
    t.equal(lex('"a\\"b"'), [Token.new(TokenKind.String, '"a\\"b"')])
    t.equal(lex('"a'), [Token.new(TokenKind.String, '"a')])
    t.equal(lex('"\\"\\""'), [Token.new(TokenKind.String, '"\\"\\""')])
    t.equal(lex("'a'"), [Token.new(TokenKind.String, "'a'")])
    t.equal(lex("'foo'"), [Token.new(TokenKind.String, "'foo'")])
  })

  t.test('Line comments', fn (t) {
    t.equal(lex('//'), [Token.new(TokenKind.Comment, '//')])
    t.equal(lex('// abc'), [Token.new(TokenKind.Comment, '// abc')])
    t.equal(
      lex('// abc\n'),
      [Token.new(TokenKind.Comment, '// abc'), Token.new(TokenKind.Text, '\n')],
    )
  })

  t.test('Block comments', fn (t) {
    t.equal(lex('/* abc */'), [Token.new(TokenKind.Comment, '/* abc */')])
    t.equal(
      lex('/* a */b'),
      [Token.new(TokenKind.Comment, '/* a */'), Token.new(TokenKind.Text, 'b')],
    )
  })

  t.test('Numbers', fn (t) {
    t.equal(lex('10'), [Token.new(TokenKind.Int, '10')])
    t.equal(lex('0x123'), [Token.new(TokenKind.Int, '0x123')])
    t.equal(lex('0X123'), [Token.new(TokenKind.Int, '0X123')])
    t.equal(lex('1_2'), [Token.new(TokenKind.Int, '1_2')])
    t.equal(lex('10.2'), [Token.new(TokenKind.Float, '10.2')])
    t.equal(lex('10e2'), [Token.new(TokenKind.Float, '10e2')])
    t.equal(lex('10E2'), [Token.new(TokenKind.Float, '10E2')])
    t.equal(lex('10e+2'), [Token.new(TokenKind.Float, '10e+2')])
    t.equal(lex('10E+2'), [Token.new(TokenKind.Float, '10E+2')])
    t.equal(lex('10e-2'), [Token.new(TokenKind.Float, '10e-2')])
    t.equal(lex('10E-2'), [Token.new(TokenKind.Float, '10E-2')])
    t.equal(lex('0x5D'), [Token.new(TokenKind.Int, '0x5D')])
    t.equal(lex('0x1aF'), [Token.new(TokenKind.Int, '0x1aF')])
    t.equal(
      lex('10e+'),
      [
        Token.new(TokenKind.Int, '10'),
        Token.new(TokenKind.Text, 'e'),
        Token.new(TokenKind.Text, '+'),
      ],
    )
    t.equal(
      lex('10.a'),
      [
        Token.new(TokenKind.Int, '10'),
        Token.new(TokenKind.Text, '.'),
        Token.new(TokenKind.Text, 'a'),
      ],
    )
  })

  t.test('Symbols', fn (t) {
    [
      '~',
      '!',
      '$',
      '%',
      '^',
      '*',
      '(',
      ')',
      '_',
      '+',
      '`',
      '-',
      '=',
      '{',
      '}',
      '[',
      ']',
      '<',
      '>',
      ',',
      '.',
      '/',
      '?',
      '\\',
      '|',
    ]
      .into_iter
      .each(fn (sym) { t.equal(lex(sym), [Token.new(TokenKind.Text, sym)]) })
  })

  t.test('Identifiers and keywords', fn (t) {
    t.equal(lex('foo'), [Token.new(TokenKind.Text, 'foo')])
    t.equal(lex('foo_bar'), [Token.new(TokenKind.Text, 'foo_bar')])
    t.equal(lex('foo10'), [Token.new(TokenKind.Text, 'foo10')])
    t.equal(
      lex('foo-'),
      [Token.new(TokenKind.Text, 'foo'), Token.new(TokenKind.Text, '-')],
    )

    KEYWORDS.iter.each(fn (kw) {
      t.equal(lex(kw), [Token.new(TokenKind.Keyword, kw)])
    })

    t.equal(lex('&'), [Token.new(TokenKind.Keyword, '&')])
  })

  t.test('Whitespace', fn (t) {
    t.equal(lex(' '), [Token.new(TokenKind.Text, ' ')])
    t.equal(lex('\t'), [Token.new(TokenKind.Text, '\t')])
    t.equal(lex('\n'), [Token.new(TokenKind.Text, '\n')])
    t.equal(lex('\r'), [Token.new(TokenKind.Text, '\r')])
  })

  t.test('Invalid syntax', fn (t) {
    t.equal(lex('ééé'), [Token.new(TokenKind.Text, 'ééé')])
    t.equal(lex('\0'), [Token.new(TokenKind.Text, '\0')])
    t.equal(lex('\u{1}'), [Token.new(TokenKind.Text, '\u{1}')])
  })

  t.test('Constants', fn (t) {
    t.equal(lex('Foo'), [Token.new(TokenKind.Text, 'Foo')])
    t.equal(lex('Foo_Bar'), [Token.new(TokenKind.Text, 'Foo_Bar')])
  })

  t.test('Macros', fn (t) {
    t.equal(
      lex('#a '),
      [Token.new(TokenKind.Macro, '#a'), Token.new(TokenKind.Text, ' ')],
    )

    t.equal(lex('##a'), [Token.new(TokenKind.Macro, '##a')])
    t.equal(lex('#define'), [Token.new(TokenKind.Macro, '#define')])
    t.equal(lex('#include'), [Token.new(TokenKind.Macro, '#include')])
  })
}
