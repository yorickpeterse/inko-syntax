import std.test.Tests
import syntax.(Token, TokenKind)
import syntax.lexer.rust.(KEYWORDS, Lexer)

fn lex(input: String) -> Array[Token] {
  Lexer.new(input.to_byte_array).to_array
}

fn pub tests(t: mut Tests) {
  t.test('Strings') fn (t) {
    t.equal(lex('""'), [Token.new(TokenKind.String, 0, 2)])
    t.equal(lex('"foo"'), [Token.new(TokenKind.String, 0, 5)])
    t.equal(lex('"a\\"b"'), [Token.new(TokenKind.String, 0, 6)])
    t.equal(lex('"a'), [Token.new(TokenKind.String, 0, 2)])
    t.equal(lex('"\\"\\""'), [Token.new(TokenKind.String, 0, 6)])
  }

  t.test('Character literals') fn (t) {
    t.equal(lex("'a"), [Token.new(TokenKind.Text, 0, 2)])
    t.equal(
      lex("'a,"),
      [Token.new(TokenKind.Text, 0, 2), Token.new(TokenKind.Text, 2, 1)]
    )

    t.equal(lex("'a'"), [Token.new(TokenKind.String, 0, 3)])
    t.equal(lex("'foo'"), [Token.new(TokenKind.String, 0, 5)])
  }

  t.test('Line comments') fn (t) {
    t.equal(lex('//'), [Token.new(TokenKind.Comment, 0, 2)])
    t.equal(lex('// abc'), [Token.new(TokenKind.Comment, 0, 6)])
    t.equal(
      lex("// abc\n"),
      [Token.new(TokenKind.Comment, 0, 6), Token.new(TokenKind.Text, 6, 1)]
    )
  }

  t.test('Block comments') fn (t) {
    t.equal(lex('/* abc */'), [Token.new(TokenKind.Comment, 0, 9)])
    t.equal(
      lex('/* a */b'),
      [Token.new(TokenKind.Comment, 0, 7), Token.new(TokenKind.Text, 7, 1)]
    )
  }

  t.test('Numbers') fn (t) {
    t.equal(lex('10'), [Token.new(TokenKind.Int, 0, 2)])
    t.equal(lex('1_2'), [Token.new(TokenKind.Int, 0, 3)])
    t.equal(lex('10.2'), [Token.new(TokenKind.Float, 0, 4)])
    t.equal(lex('10e2'), [Token.new(TokenKind.Float, 0, 4)])
    t.equal(lex('10E2'), [Token.new(TokenKind.Float, 0, 4)])
    t.equal(lex('10e+2'), [Token.new(TokenKind.Float, 0, 5)])
    t.equal(lex('10E+2'), [Token.new(TokenKind.Float, 0, 5)])
    t.equal(lex('10e-2'), [Token.new(TokenKind.Float, 0, 5)])
    t.equal(lex('10E-2'), [Token.new(TokenKind.Float, 0, 5)])
    t.equal(
      lex('10e+'),
      [
        Token.new(TokenKind.Int, 0, 2),
        Token.new(TokenKind.Text, 2, 1),
        Token.new(TokenKind.Text, 3, 1),
      ]
    )
  }

  t.test('Symbols') fn (t) {
    [
      '~', '!', '$', '%', '^', '*', '(', ')', '_', '+', '`', '-', '=', '{', '}',
      '[', ']', '<', '>', ',', '.', '/', '?', '\\', '|',
    ].into_iter.each fn (sym) {
      t.equal(lex(sym), [Token.new(TokenKind.Text, 0, 1)])
    }
  }

  t.test('Identifiers and keywords') fn (t) {
    t.equal(lex('foo'), [Token.new(TokenKind.Text, 0, 3)])
    t.equal(lex('foo_bar'), [Token.new(TokenKind.Text, 0, 7)])
    t.equal(lex('foo10'), [Token.new(TokenKind.Text, 0, 5)])
    t.equal(
      lex('foo-'),
      [Token.new(TokenKind.Text, 0, 3), Token.new(TokenKind.Text, 3, 1)]
    )

    KEYWORDS.iter.each fn (kw) {
      t.equal(lex(kw), [Token.new(TokenKind.Keyword, 0, kw.size)])
    }

    t.equal(lex('&'), [Token.new(TokenKind.Keyword, 0, 1)])
  }

  t.test('Whitespace') fn (t) {
    t.equal(lex(' '), [Token.new(TokenKind.Text, 0, 1)])
    t.equal(lex("\t"), [Token.new(TokenKind.Text, 0, 1)])
    t.equal(lex("\n"), [Token.new(TokenKind.Text, 0, 1)])
    t.equal(lex("\r"), [Token.new(TokenKind.Text, 0, 1)])
  }

  t.test('Invalid syntax') fn (t) {
    t.equal(lex('ééé'), [Token.new(TokenKind.Text, 0, 6)])
    t.equal(lex("\0"), [Token.new(TokenKind.Text, 0, 1)])
    t.equal(lex("\u{0001}"), [Token.new(TokenKind.Text, 0, 1)])
  }

  t.test('Constants') fn (t) {
    t.equal(lex('Foo'), [Token.new(TokenKind.Text, 0, 3)])
    t.equal(lex('Foo_Bar'), [Token.new(TokenKind.Text, 0, 7)])
  }

  t.test('Macros') fn (t) {
    t.equal(lex('matches!'), [Token.new(TokenKind.Macro, 0, 8)])
    t.equal(lex('async!'), [Token.new(TokenKind.Macro, 0, 6)])
  }
}
