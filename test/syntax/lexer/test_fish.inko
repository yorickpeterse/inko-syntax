import std.test.Tests
import syntax.(Token, TokenKind)
import syntax.lexer.fish.(KEYWORDS, Lexer)

fn lex(input: String) -> Array[Token] {
  Lexer.new(input.to_byte_array).to_array
}

fn pub tests(t: mut Tests) {
  t.test('Strings') fn (t) {
    t.equal(lex('""'), [Token.new(TokenKind.String, '""')])
    t.equal(lex('"foo"'), [Token.new(TokenKind.String, '"foo"')])
    t.equal(lex('"a\\"b"'), [Token.new(TokenKind.String, '"a\\"b"')])
    t.equal(lex("''"), [Token.new(TokenKind.String, "''")])
    t.equal(lex("'foo'"), [Token.new(TokenKind.String, "'foo'")])
    t.equal(lex("'a\\'b'"), [Token.new(TokenKind.String, "'a\\'b'")])
    t.equal(lex('"a'), [Token.new(TokenKind.String, '"a')])
    t.equal(lex("'a"), [Token.new(TokenKind.String, "'a")])
  }

  t.test('Comments') fn (t) {
    t.equal(lex('#'), [Token.new(TokenKind.Comment, '#')])
    t.equal(lex('# abc'), [Token.new(TokenKind.Comment, '# abc')])
    t.equal(
      lex("# abc\n"),
      [Token.new(TokenKind.Comment, '# abc'), Token.new(TokenKind.Text, "\n")]
    )
  }

  t.test('Numbers') fn (t) {
    t.equal(lex('10'), [Token.new(TokenKind.Int, '10')])
    t.equal(
      lex('10.5'), [
        Token.new(TokenKind.Int, '10'),
        Token.new(TokenKind.Text, '.'),
        Token.new(TokenKind.Int, '5'),
      ]
    )
  }

  t.test('Symbols') fn (t) {
    [
      '~', '!', '$', '%', '^', '&', '*', '(', ')', '_', '+', '`', '-', '=', '{',
      '}', '[', ']', '<', '>', ',', '.', '/', '?', '\\', '|',
    ].into_iter.each fn (sym) {
      t.equal(lex(sym), [Token.new(TokenKind.Text, sym)])
    }
  }

  t.test('Identifiers and keywords') fn (t) {
    t.equal(lex('foo'), [Token.new(TokenKind.Text, 'foo')])
    t.equal(lex('foo_bar'), [Token.new(TokenKind.Text, 'foo_bar')])
    t.equal(lex('foo10'), [Token.new(TokenKind.Text, 'foo10')])
    t.equal(
      lex('foo-'),
      [Token.new(TokenKind.Text, 'foo'), Token.new(TokenKind.Text, '-')]
    )

    t.equal(
      lex('foo-_'),
      [
        Token.new(TokenKind.Text, 'foo'),
        Token.new(TokenKind.Text, '-'),
        Token.new(TokenKind.Text, '_')
      ]
    )

    t.equal(lex('foo-10'), [Token.new(TokenKind.Text, 'foo-10')])

    KEYWORDS.iter.each fn (kw) {
      t.equal(lex(kw), [Token.new(TokenKind.Keyword, kw)])
    }
  }

  t.test('Whitespace') fn (t) {
    t.equal(lex(' '), [Token.new(TokenKind.Text, ' ')])
    t.equal(lex("\t"), [Token.new(TokenKind.Text, "\t")])
    t.equal(lex("\n"), [Token.new(TokenKind.Text, "\n")])
    t.equal(lex("\r"), [Token.new(TokenKind.Text, "\r")])
  }

  t.test('Invalid syntax') fn (t) {
    t.equal(lex('ééé'), [Token.new(TokenKind.Text, 'ééé')])
    t.equal(lex("\0"), [Token.new(TokenKind.Text, "\0")])
    t.equal(lex("\u{0001}"), [Token.new(TokenKind.Text, "\u{0001}")])
  }
}
