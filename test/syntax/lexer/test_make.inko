import std.test (Tests)
import syntax (Token, TokenKind)
import syntax.lexer.make (KEYWORDS, Lexer, TARGETS)

fn lex(input: String) -> Array[Token] {
  Lexer.new(input.to_byte_array).to_array
}

fn pub tests(t: mut Tests) {
  t.test('Strings', fn (t) {
    t.equal(lex('""'), [Token.new(TokenKind.String, '""')])
    t.equal(lex('"foo"'), [Token.new(TokenKind.String, '"foo"')])
    t.equal(lex('"a\\"b"'), [Token.new(TokenKind.String, '"a\\"b"')])
    t.equal(lex("''"), [Token.new(TokenKind.String, "''")])
    t.equal(lex("'foo'"), [Token.new(TokenKind.String, "'foo'")])
    t.equal(lex("'a\\'b'"), [Token.new(TokenKind.String, "'a\\'b'")])
    t.equal(lex('"a'), [Token.new(TokenKind.String, '"a')])
    t.equal(lex("'a"), [Token.new(TokenKind.String, "'a")])
  })

  t.test('Comments', fn (t) {
    t.equal(lex('#'), [Token.new(TokenKind.Comment, '#')])
    t.equal(lex('# abc'), [Token.new(TokenKind.Comment, '# abc')])
    t.equal(
      lex('# abc\n'),
      [Token.new(TokenKind.Comment, '# abc'), Token.new(TokenKind.Text, '\n')],
    )
  })

  t.test('Numbers', fn (t) {
    t.equal(lex('10'), [Token.new(TokenKind.Int, '10')])
    t.equal(
      lex('10.5'),
      [
        Token.new(TokenKind.Int, '10'),
        Token.new(TokenKind.Text, '.'),
        Token.new(TokenKind.Int, '5'),
      ],
    )
  })

  t.test('Symbols', fn (t) {
    for
      sym
    in
      [
        '~',
        '!',
        '$',
        '%',
        '^',
        '&',
        '*',
        '(',
        ')',
        '_',
        '+',
        '`',
        '-',
        '=',
        '{',
        '}',
        '[',
        ']',
        '<',
        '>',
        ',',
        '.',
        '/',
        '?',
        '\\',
        '|',
      ]
    {
      t.equal(lex(sym), [Token.new(TokenKind.Text, sym)])
    }
  })

  t.test('Identifiers and keywords', fn (t) {
    t.equal(lex('foo'), [Token.new(TokenKind.Text, 'foo')])
    t.equal(lex('foo_bar'), [Token.new(TokenKind.Text, 'foo_bar')])
    t.equal(lex('foo10'), [Token.new(TokenKind.Text, 'foo10')])
    t.equal(
      lex('foo-'),
      [Token.new(TokenKind.Text, 'foo'), Token.new(TokenKind.Text, '-')],
    )

    for kw in KEYWORDS.iter {
      t.equal(lex(kw), [Token.new(TokenKind.Keyword, kw)])
    }

    for name in TARGETS.iter {
      t.equal(lex(name), [Token.new(TokenKind.Keyword, name)])
    }

    t.equal(lex('.PHO.NY'), [Token.new(TokenKind.Text, '.PHO.NY')])
  })

  t.test('Whitespace', fn (t) {
    t.equal(lex(' '), [Token.new(TokenKind.Text, ' ')])
    t.equal(lex('\t'), [Token.new(TokenKind.Text, '\t')])
    t.equal(lex('\n'), [Token.new(TokenKind.Text, '\n')])
    t.equal(lex('\r'), [Token.new(TokenKind.Text, '\r')])
  })

  t.test('Invalid syntax', fn (t) {
    t.equal(lex('ééé'), [Token.new(TokenKind.Text, 'ééé')])
    t.equal(lex('\0'), [Token.new(TokenKind.Text, '\0')])
    t.equal(lex('\u{1}'), [Token.new(TokenKind.Text, '\u{1}')])
  })
}
